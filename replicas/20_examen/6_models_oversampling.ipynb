{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from utils.balance_data import Oversampler\n",
    "from utils.base_models import BaseModels\n",
    "from utils.numerical_scalers import NumericalScalers\n",
    "from utils.categorical_encoders import CategoricalEncoders\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13056, 14)\n",
      "TARGET\n",
      "0    6978\n",
      "1    6078\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Leer el dataset\n",
    "dataset = pd.read_parquet('./data/2_data_preprocesada.parquet')\n",
    "dataset = dataset.drop(columns=['DF_TYPE'])\n",
    "\n",
    "# Seleccionar aleatoriamente el 10% de los datos\n",
    "dataset = dataset.sample(frac=0.01, random_state=42)\n",
    "print(dataset.shape)\n",
    "print(dataset['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generar combinaciones de categóricos y numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separar TARGET, df_categorical y df_numeric\n",
    "df_target = dataset['TARGET']\n",
    "df_categorical = dataset.select_dtypes(include=['object'])\n",
    "df_numeric = dataset.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# 3. Eliminar TARGET de df_numeric\n",
    "df_numeric = df_numeric.drop(columns=['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Encoder: LabelEncoder - Scaler: StandardScaler - Shape: (13056, 13)\n",
      "01 -> Encoder: LabelEncoder - Scaler: MinMaxScaler - Shape: (13056, 13)\n",
      "02 -> Encoder: LabelEncoder - Scaler: MaxAbsScaler - Shape: (13056, 13)\n",
      "03 -> Encoder: LabelEncoder - Scaler: RobustScaler - Shape: (13056, 13)\n",
      "04 -> Encoder: LabelEncoder - Scaler: Normalizer - Shape: (13056, 13)\n",
      "05 -> Encoder: LabelEncoder - Scaler: PowerTransformer - Shape: (13056, 13)\n",
      "06 -> Encoder: OneHotEncoder - Scaler: StandardScaler - Shape: (13056, 38)\n",
      "07 -> Encoder: OneHotEncoder - Scaler: MinMaxScaler - Shape: (13056, 38)\n",
      "08 -> Encoder: OneHotEncoder - Scaler: MaxAbsScaler - Shape: (13056, 38)\n",
      "09 -> Encoder: OneHotEncoder - Scaler: RobustScaler - Shape: (13056, 38)\n",
      "10 -> Encoder: OneHotEncoder - Scaler: Normalizer - Shape: (13056, 38)\n",
      "11 -> Encoder: OneHotEncoder - Scaler: PowerTransformer - Shape: (13056, 38)\n",
      "12 -> Encoder: OrdinalEncoder - Scaler: StandardScaler - Shape: (13056, 13)\n",
      "13 -> Encoder: OrdinalEncoder - Scaler: MinMaxScaler - Shape: (13056, 13)\n",
      "14 -> Encoder: OrdinalEncoder - Scaler: MaxAbsScaler - Shape: (13056, 13)\n",
      "15 -> Encoder: OrdinalEncoder - Scaler: RobustScaler - Shape: (13056, 13)\n",
      "16 -> Encoder: OrdinalEncoder - Scaler: Normalizer - Shape: (13056, 13)\n",
      "17 -> Encoder: OrdinalEncoder - Scaler: PowerTransformer - Shape: (13056, 13)\n",
      "18 -> Encoder: FrequencyEncoder - Scaler: StandardScaler - Shape: (13056, 13)\n",
      "19 -> Encoder: FrequencyEncoder - Scaler: MinMaxScaler - Shape: (13056, 13)\n",
      "20 -> Encoder: FrequencyEncoder - Scaler: MaxAbsScaler - Shape: (13056, 13)\n",
      "21 -> Encoder: FrequencyEncoder - Scaler: RobustScaler - Shape: (13056, 13)\n",
      "22 -> Encoder: FrequencyEncoder - Scaler: Normalizer - Shape: (13056, 13)\n",
      "23 -> Encoder: FrequencyEncoder - Scaler: PowerTransformer - Shape: (13056, 13)\n",
      "24 -> Encoder: BinaryEncoder - Scaler: StandardScaler - Shape: (13056, 24)\n",
      "25 -> Encoder: BinaryEncoder - Scaler: MinMaxScaler - Shape: (13056, 24)\n",
      "26 -> Encoder: BinaryEncoder - Scaler: MaxAbsScaler - Shape: (13056, 24)\n",
      "27 -> Encoder: BinaryEncoder - Scaler: RobustScaler - Shape: (13056, 24)\n",
      "28 -> Encoder: BinaryEncoder - Scaler: Normalizer - Shape: (13056, 24)\n",
      "29 -> Encoder: BinaryEncoder - Scaler: PowerTransformer - Shape: (13056, 24)\n",
      "30 -> Encoder: BackwardDifferenceEncoder - Scaler: StandardScaler - Shape: (13056, 39)\n",
      "31 -> Encoder: BackwardDifferenceEncoder - Scaler: MinMaxScaler - Shape: (13056, 39)\n",
      "32 -> Encoder: BackwardDifferenceEncoder - Scaler: MaxAbsScaler - Shape: (13056, 39)\n",
      "33 -> Encoder: BackwardDifferenceEncoder - Scaler: RobustScaler - Shape: (13056, 39)\n",
      "34 -> Encoder: BackwardDifferenceEncoder - Scaler: Normalizer - Shape: (13056, 39)\n",
      "35 -> Encoder: BackwardDifferenceEncoder - Scaler: PowerTransformer - Shape: (13056, 39)\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar las combinaciones de Encoder y Scaler\n",
    "encoder_methods = ['LabelEncoder', 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', 'BinaryEncoder', 'BackwardDifferenceEncoder']\n",
    "scaler_methods = ['StandardScaler', 'MinMaxScaler', 'MaxAbsScaler', 'RobustScaler', 'Normalizer', 'PowerTransformer']\n",
    "\n",
    "# 5. Instanciar CategoricalEncoders\n",
    "categorical = CategoricalEncoders(dataset=df_categorical)\n",
    "binary_columns, categorical_columns = categorical.get_binary_categorical_columns()\n",
    "\n",
    "# 6. Instanciar NumericalScalers\n",
    "numerical = NumericalScalers(dataset=df_numeric)\n",
    "\n",
    "# 7. Generar todas las combinaciones de Encoder y Scaler\n",
    "def get_list_data_processed(encoder_methods, scaler_methods):\n",
    "    combinations = list(itertools.product(encoder_methods, scaler_methods))\n",
    "    \n",
    "    i=0\n",
    "    list_data_processed = []\n",
    "    for encoder_method, scaler_method in combinations:\n",
    "        data_encoded = categorical.provider(binary_columns, categorical_columns, method=encoder_method)\n",
    "        data_scaled = numerical.provider(method=scaler_method)\n",
    "\n",
    "        processed_data = pd.concat([data_encoded, data_scaled], axis=1)\n",
    "        list_data_processed.append((f'{encoder_method} - {scaler_method}', processed_data))\n",
    "        print(f'{str(i).zfill(2)} -> Encoder: {encoder_method} - Scaler: {scaler_method} - Shape: {processed_data.shape}')\n",
    "        i+=1\n",
    "    \n",
    "    return list_data_processed\n",
    "\n",
    "list_data_processed = get_list_data_processed(encoder_methods, scaler_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split test-train y balanceo de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Split Encoded Scaler Method: LabelEncoder - StandardScaler\n",
      "01 -> Split Encoded Scaler Method: LabelEncoder - MinMaxScaler\n",
      "02 -> Split Encoded Scaler Method: LabelEncoder - MaxAbsScaler\n",
      "03 -> Split Encoded Scaler Method: LabelEncoder - RobustScaler\n",
      "04 -> Split Encoded Scaler Method: LabelEncoder - Normalizer\n",
      "05 -> Split Encoded Scaler Method: LabelEncoder - PowerTransformer\n",
      "06 -> Split Encoded Scaler Method: OneHotEncoder - StandardScaler\n",
      "07 -> Split Encoded Scaler Method: OneHotEncoder - MinMaxScaler\n",
      "08 -> Split Encoded Scaler Method: OneHotEncoder - MaxAbsScaler\n",
      "09 -> Split Encoded Scaler Method: OneHotEncoder - RobustScaler\n",
      "10 -> Split Encoded Scaler Method: OneHotEncoder - Normalizer\n",
      "11 -> Split Encoded Scaler Method: OneHotEncoder - PowerTransformer\n",
      "12 -> Split Encoded Scaler Method: OrdinalEncoder - StandardScaler\n",
      "13 -> Split Encoded Scaler Method: OrdinalEncoder - MinMaxScaler\n",
      "14 -> Split Encoded Scaler Method: OrdinalEncoder - MaxAbsScaler\n",
      "15 -> Split Encoded Scaler Method: OrdinalEncoder - RobustScaler\n",
      "16 -> Split Encoded Scaler Method: OrdinalEncoder - Normalizer\n",
      "17 -> Split Encoded Scaler Method: OrdinalEncoder - PowerTransformer\n",
      "18 -> Split Encoded Scaler Method: FrequencyEncoder - StandardScaler\n",
      "19 -> Split Encoded Scaler Method: FrequencyEncoder - MinMaxScaler\n",
      "20 -> Split Encoded Scaler Method: FrequencyEncoder - MaxAbsScaler\n",
      "21 -> Split Encoded Scaler Method: FrequencyEncoder - RobustScaler\n",
      "22 -> Split Encoded Scaler Method: FrequencyEncoder - Normalizer\n",
      "23 -> Split Encoded Scaler Method: FrequencyEncoder - PowerTransformer\n",
      "24 -> Split Encoded Scaler Method: BinaryEncoder - StandardScaler\n",
      "25 -> Split Encoded Scaler Method: BinaryEncoder - MinMaxScaler\n",
      "26 -> Split Encoded Scaler Method: BinaryEncoder - MaxAbsScaler\n",
      "27 -> Split Encoded Scaler Method: BinaryEncoder - RobustScaler\n",
      "28 -> Split Encoded Scaler Method: BinaryEncoder - Normalizer\n",
      "29 -> Split Encoded Scaler Method: BinaryEncoder - PowerTransformer\n",
      "30 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - StandardScaler\n",
      "31 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MinMaxScaler\n",
      "32 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MaxAbsScaler\n",
      "33 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - RobustScaler\n",
      "34 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - Normalizer\n",
      "35 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - PowerTransformer\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar los conjuntos de entrenamiento y prueba\n",
    "def get_list_split_data(list_data_processed):\n",
    "    i=0\n",
    "    list_split_data = []\n",
    "    for encoded_scaler, data_processed in list_data_processed:\n",
    "        X = data_processed\n",
    "        y = df_target\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        list_split_data.append((encoded_scaler, X_train, X_test, y_train, y_test))\n",
    "        print(f'{str(i).zfill(2)} -> Split Encoded Scaler Method: {encoded_scaler}')\n",
    "        i+=1\n",
    "\n",
    "    return list_split_data\n",
    "\n",
    "list_data_encoded_split = get_list_split_data(list_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00: Encoded method: LabelEncoder - StandardScaler - Oversampler method: RandomOverSampler - Train shape: (11164, 13) - Test shape: (2612, 13)\n",
      "01: Encoded method: LabelEncoder - StandardScaler - Oversampler method: SMOTE - Train shape: (11164, 13) - Test shape: (2612, 13)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Method should be 'RandomOverSampler', 'SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SVMSMOTE', or 'KMeansSMOTE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m list_data_encoded_balanced\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Aplicar el balanceo a los datos de entrenamiento\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m list_data_encoded_split_balanced \u001b[38;5;241m=\u001b[39m \u001b[43mget_list_data_encoded_balanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_data_encoded_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m _, _, X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m list_data_encoded_split_balanced[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124my_train:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mvalue_counts()))\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mget_list_data_encoded_balanced\u001b[0;34m(list_data_encoded_split)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoded_method, X_train, X_test, y_train, y_test \u001b[38;5;129;01min\u001b[39;00m list_data_encoded_split:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m oversampler_method \u001b[38;5;129;01min\u001b[39;00m oversampler_methods:\n\u001b[0;32m---> 11\u001b[0m         X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m \u001b[43moversampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moversampler_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         list_data_encoded_balanced\u001b[38;5;241m.\u001b[39mappend((encoded_method, \n\u001b[1;32m     14\u001b[0m                                            oversampler_method, \n\u001b[1;32m     15\u001b[0m                                            X_train_balanced, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m                                            y_test\n\u001b[1;32m     19\u001b[0m                                            ))\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Encoded method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Oversampler method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moversampler_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_balanced\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/GitHubProjects/laughing-adventure/replicas/20_examen/utils/balance_data.py:49\u001b[0m, in \u001b[0;36mOversampler.provider\u001b[0;34m(self, method, X, y)\u001b[0m\n\u001b[1;32m     47\u001b[0m     resampler \u001b[38;5;241m=\u001b[39m KMeansSMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod should be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomOverSampler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMOTE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADASYN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBorderlineSMOTE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVMSMOTE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKMeansSMOTE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m resampler\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_resampled, y_resampled\n",
      "\u001b[0;31mValueError\u001b[0m: Method should be 'RandomOverSampler', 'SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SVMSMOTE', or 'KMeansSMOTE'"
     ]
    }
   ],
   "source": [
    "# 5. Aplicar balanceo de clases a cada list_data_encoded\n",
    "oversampler = Oversampler()\n",
    "\n",
    "def get_list_data_encoded_balanced(list_data_encoded_split):\n",
    "    oversampler_methods = ['RandomOverSampler', 'SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SVMSMOTE']\n",
    "    \n",
    "    i=0\n",
    "    list_data_encoded_balanced = []\n",
    "    for encoded_method, X_train, X_test, y_train, y_test in list_data_encoded_split:\n",
    "        for oversampler_method in oversampler_methods:\n",
    "            X_train_balanced, y_train_balanced = oversampler.provider(method=oversampler_method, X=X_train, y=y_train)\n",
    "            \n",
    "            list_data_encoded_balanced.append((encoded_method, \n",
    "                                               oversampler_method, \n",
    "                                               X_train_balanced, \n",
    "                                               X_test,\n",
    "                                               y_train_balanced,\n",
    "                                               y_test\n",
    "                                               ))\n",
    "            print(f'{str(i).zfill(2)}: Encoded method: {encoded_method} - Oversampler method: {oversampler_method} - Train shape: {X_train_balanced.shape} - Test shape: {X_test.shape}')\n",
    "            i += 1\n",
    "\n",
    "    return list_data_encoded_balanced\n",
    "\n",
    "# Aplicar el balanceo a los datos de entrenamiento\n",
    "list_data_encoded_split_balanced = get_list_data_encoded_balanced(list_data_encoded_split)\n",
    "\n",
    "_, _, X_train, X_test, y_train, y_test = list_data_encoded_split_balanced[0]\n",
    "print('\\ny_train:', str(y_train.value_counts()))\n",
    "print('\\ny_test:', str(y_test.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Obtener las columnas binarias y categóricas\n",
    "categorical = CategoricalEncoders(dataset=dataset)\n",
    "binary_columns, categorical_columns = categorical.get_binary_categorical_columns()\n",
    "\n",
    "# 3. Obtener los datos codificados\n",
    "def get_list_data_encoded():\n",
    "    encoded_methods = ['LabelEncoder', 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', 'BinaryEncoder', 'BackwardDifferenceEncoder']\n",
    "\n",
    "    i = 0\n",
    "    list_data_encoded = []\n",
    "    for encoded_method in encoded_methods:\n",
    "        data_encoded = categorical.provider(binary_columns, categorical_columns, method=encoded_method)\n",
    "        list_data_encoded.append((encoded_method, data_encoded))\n",
    "        print(f'{str(i).zfill(2)}: Encoded method: {encoded_method} - Data shape: {data_encoded.shape}')\n",
    "        i += 1\n",
    "\n",
    "    return list_data_encoded\n",
    "\n",
    "list_data_encoded = get_list_data_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_list_data_encoded_split(list_data_encoded, column_target_name, test_size=0.2, random_state=42):\n",
    "    \n",
    "    i=0\n",
    "    list_data_split = []\n",
    "    for method, data in list_data_encoded:\n",
    "        X = data.drop(columns=[column_target_name])\n",
    "        y = data[column_target_name]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        list_data_split.append((method, X_train, X_test, y_train, y_test))\n",
    "\n",
    "        print(f'{str(i).zfill(2)}: Method: {method} - Train shape: {X_train.shape}, Test shape: {X_test.shape}')\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    return list_data_split\n",
    "\n",
    "# Uso de la función\n",
    "list_data_encoded_split = get_list_data_encoded_split(list_data_encoded, column_target_name='Target')\n",
    "\n",
    "_, X_train, X_test, y_train, y_test = list_data_encoded_split[0]\n",
    "print('\\ny_train:', str(y_train.value_counts()))\n",
    "print('\\ny_test:', str(y_test.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Aplicar balanceo de clases a cada list_data_encoded\n",
    "oversampler = Oversampler()\n",
    "\n",
    "def get_list_data_encoded_balanced(list_data_encoded_split):\n",
    "    oversampler_methods = ['random', 'smote', 'adasyn', 'borderlinesmote', 'svmsmote']\n",
    "    \n",
    "    i=0\n",
    "    list_data_encoded_balanced = []\n",
    "    for encoded_method, X_train, X_test, y_train, y_test in list_data_encoded_split:\n",
    "        for oversampler_method in oversampler_methods:\n",
    "            X_train_balanced, y_train_balanced = oversampler.provider(method=oversampler_method, X=X_train, y=y_train)\n",
    "            \n",
    "            list_data_encoded_balanced.append((encoded_method, \n",
    "                                               oversampler_method, \n",
    "                                               X_train_balanced, \n",
    "                                               X_test,\n",
    "                                               y_train_balanced,\n",
    "                                               y_test\n",
    "                                               ))\n",
    "            print(f'{str(i).zfill(2)}: Encoded method: {encoded_method} - Oversampler method: {oversampler_method} - Train shape: {X_train_balanced.shape} - Test shape: {X_test.shape}')\n",
    "            i += 1\n",
    "\n",
    "    return list_data_encoded_balanced\n",
    "\n",
    "# Aplicar el balanceo a los datos de entrenamiento\n",
    "list_data_encoded_split_balanced = get_list_data_encoded_balanced(list_data_encoded_split)\n",
    "\n",
    "_, _, X_train, X_test, y_train, y_test = list_data_encoded_split_balanced[0]\n",
    "print('\\ny_train:', str(y_train.value_counts()))\n",
    "print('\\ny_test:', str(y_test.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from utils.base_models import BaseModels\n",
    "\n",
    "# Inicializar BaseModels y definir los nombres de los modelos\n",
    "base_models = BaseModels()\n",
    "name_models = ['logistic_regression', 'decision_tree', 'random_forest',\n",
    "               'gradient_boosting', 'svm', 'knn', 'naive_bayes', 'mlp',\n",
    "               'lgbm', 'catboost', 'xgboost']\n",
    "\n",
    "i = 0\n",
    "all_results = []\n",
    "for encoded_method, oversampler_method, X_train, X_test, y_train, y_test in list_data_encoded_split_balanced:\n",
    "    results = []\n",
    "    for name in name_models:\n",
    "        model = base_models.provider(name)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predict_train = model.predict_proba(X_train)[:, 1]\n",
    "        predict_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        train_auc = roc_auc_score(y_train, predict_train)\n",
    "        test_auc = roc_auc_score(y_test, predict_test)\n",
    "\n",
    "        results.append((name, train_auc, test_auc))\n",
    "\n",
    "        print(f\"{str(i).zfill(2)}: Encoded Method: {encoded_method} - Oversampler Method: {oversampler_method} - AUC on training data with {name}: {train_auc:.3f}\")\n",
    "        print(f\"AUC on testing data with {name}: {test_auc:.3f}\")\n",
    "        i += 1\n",
    "\n",
    "    # Ordenar los resultados por test_auc de mayor a menor\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Desempaquetar los resultados ordenados y almacenar con el método de codificación y balanceo\n",
    "    sorted_names, train_aucs, test_aucs = zip(*results)\n",
    "    all_results.append((encoded_method, oversampler_method, sorted_names, train_aucs, test_aucs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Consolidar todos los resultados en una lista\n",
    "consolidated_results = []\n",
    "for encoded_method, oversampler_method, sorted_names, train_aucs, test_aucs in all_results:\n",
    "    for name, train_auc, test_auc in zip(sorted_names, train_aucs, test_aucs):\n",
    "        full_model_name = f\"{encoded_method} - {oversampler_method} - {name}\"\n",
    "        consolidated_results.append((full_model_name, train_auc, test_auc))\n",
    "\n",
    "# Ordenar los resultados por test_auc de mayor a menor y seleccionar el top 10\n",
    "consolidated_results.sort(key=lambda x: x[2], reverse=False)\n",
    "top_results = consolidated_results[:15]\n",
    "\n",
    "# Desempaquetar los resultados del top 10\n",
    "top_names, top_train_aucs, top_10_test_aucs = zip(*top_results)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "# Gráfico de líneas para el top 10\n",
    "plt.plot(top_names, top_train_aucs, label='Training AUC', marker='o', color='skyblue')\n",
    "plt.plot(top_names, top_10_test_aucs, label='Testing AUC', marker='o', color='salmon')\n",
    "\n",
    "# Añadir los valores a los puntos\n",
    "for i, txt in enumerate(top_train_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_names[i], top_train_aucs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "for i, txt in enumerate(top_10_test_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_names[i], top_10_test_aucs[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='red')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Top of 330 Models by Testing AUC')\n",
    "plt.xticks(rotation=90, fontsize=9)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Consolidar todos los resultados en una lista\n",
    "consolidated_results = []\n",
    "for encoded_method, oversampler_method, sorted_names, train_aucs, test_aucs in all_results:\n",
    "    for name, train_auc, test_auc in zip(sorted_names, train_aucs, test_aucs):\n",
    "        full_model_name = f\"{encoded_method} - {oversampler_method} - {name}\"\n",
    "        consolidated_results.append((full_model_name, train_auc, test_auc))\n",
    "\n",
    "# Ordenar los resultados por test_auc de mayor a menor y seleccionar el top 10\n",
    "consolidated_results.sort(key=lambda x: x[2], reverse=False)\n",
    "top_results = consolidated_results[-15:]\n",
    "\n",
    "# Desempaquetar los resultados del top 10\n",
    "top_names, top_train_aucs, top_10_test_aucs = zip(*top_results)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "# Gráfico de líneas para el top 10\n",
    "plt.plot(top_names, top_train_aucs, label='Training AUC', marker='o', color='skyblue')\n",
    "plt.plot(top_names, top_10_test_aucs, label='Testing AUC', marker='o', color='salmon')\n",
    "\n",
    "# Añadir los valores a los puntos\n",
    "for i, txt in enumerate(top_train_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_names[i], top_train_aucs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "for i, txt in enumerate(top_10_test_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_names[i], top_10_test_aucs[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='red')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Top of 330 Models by Testing AUC')\n",
    "plt.xticks(rotation=90, fontsize=9)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

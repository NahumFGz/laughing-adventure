{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from utils.base_models import BaseModels\n",
    "from utils.numerical_scalers import NumericalScalers\n",
    "from utils.categorical_encoders import CategoricalEncoders\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leer datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130560, 14)\n",
      "TARGET\n",
      "0    69943\n",
      "1    60617\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Leer el dataset\n",
    "dataset = pd.read_parquet('./data/2_data_preprocesada.parquet')\n",
    "dataset = dataset.drop(columns=['DF_TYPE'])\n",
    "\n",
    "# Seleccionar aleatoriamente el 10% de los datos\n",
    "dataset = dataset.sample(frac=0.1, random_state=42)\n",
    "print(dataset.shape)\n",
    "print(dataset['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generar combinaciones de categóricos y numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separar TARGET, df_categorical y df_numeric\n",
    "df_target = dataset['TARGET']\n",
    "df_categorical = dataset.select_dtypes(include=['object'])\n",
    "df_numeric = dataset.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# 3. Eliminar TARGET de df_numeric\n",
    "df_numeric = df_numeric.drop(columns=['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Encoder: LabelEncoder - Scaler: StandardScaler - Shape: (130560, 13)\n",
      "01 -> Encoder: LabelEncoder - Scaler: MinMaxScaler - Shape: (130560, 13)\n",
      "02 -> Encoder: LabelEncoder - Scaler: MaxAbsScaler - Shape: (130560, 13)\n",
      "03 -> Encoder: LabelEncoder - Scaler: RobustScaler - Shape: (130560, 13)\n",
      "04 -> Encoder: LabelEncoder - Scaler: Normalizer - Shape: (130560, 13)\n",
      "05 -> Encoder: LabelEncoder - Scaler: PowerTransformer - Shape: (130560, 13)\n",
      "06 -> Encoder: OneHotEncoder - Scaler: StandardScaler - Shape: (130560, 39)\n",
      "07 -> Encoder: OneHotEncoder - Scaler: MinMaxScaler - Shape: (130560, 39)\n",
      "08 -> Encoder: OneHotEncoder - Scaler: MaxAbsScaler - Shape: (130560, 39)\n",
      "09 -> Encoder: OneHotEncoder - Scaler: RobustScaler - Shape: (130560, 39)\n",
      "10 -> Encoder: OneHotEncoder - Scaler: Normalizer - Shape: (130560, 39)\n",
      "11 -> Encoder: OneHotEncoder - Scaler: PowerTransformer - Shape: (130560, 39)\n",
      "12 -> Encoder: OrdinalEncoder - Scaler: StandardScaler - Shape: (130560, 13)\n",
      "13 -> Encoder: OrdinalEncoder - Scaler: MinMaxScaler - Shape: (130560, 13)\n",
      "14 -> Encoder: OrdinalEncoder - Scaler: MaxAbsScaler - Shape: (130560, 13)\n",
      "15 -> Encoder: OrdinalEncoder - Scaler: RobustScaler - Shape: (130560, 13)\n",
      "16 -> Encoder: OrdinalEncoder - Scaler: Normalizer - Shape: (130560, 13)\n",
      "17 -> Encoder: OrdinalEncoder - Scaler: PowerTransformer - Shape: (130560, 13)\n",
      "18 -> Encoder: FrequencyEncoder - Scaler: StandardScaler - Shape: (130560, 13)\n",
      "19 -> Encoder: FrequencyEncoder - Scaler: MinMaxScaler - Shape: (130560, 13)\n",
      "20 -> Encoder: FrequencyEncoder - Scaler: MaxAbsScaler - Shape: (130560, 13)\n",
      "21 -> Encoder: FrequencyEncoder - Scaler: RobustScaler - Shape: (130560, 13)\n",
      "22 -> Encoder: FrequencyEncoder - Scaler: Normalizer - Shape: (130560, 13)\n",
      "23 -> Encoder: FrequencyEncoder - Scaler: PowerTransformer - Shape: (130560, 13)\n",
      "24 -> Encoder: BinaryEncoder - Scaler: StandardScaler - Shape: (130560, 24)\n",
      "25 -> Encoder: BinaryEncoder - Scaler: MinMaxScaler - Shape: (130560, 24)\n",
      "26 -> Encoder: BinaryEncoder - Scaler: MaxAbsScaler - Shape: (130560, 24)\n",
      "27 -> Encoder: BinaryEncoder - Scaler: RobustScaler - Shape: (130560, 24)\n",
      "28 -> Encoder: BinaryEncoder - Scaler: Normalizer - Shape: (130560, 24)\n",
      "29 -> Encoder: BinaryEncoder - Scaler: PowerTransformer - Shape: (130560, 24)\n",
      "30 -> Encoder: BackwardDifferenceEncoder - Scaler: StandardScaler - Shape: (130560, 40)\n",
      "31 -> Encoder: BackwardDifferenceEncoder - Scaler: MinMaxScaler - Shape: (130560, 40)\n",
      "32 -> Encoder: BackwardDifferenceEncoder - Scaler: MaxAbsScaler - Shape: (130560, 40)\n",
      "33 -> Encoder: BackwardDifferenceEncoder - Scaler: RobustScaler - Shape: (130560, 40)\n",
      "34 -> Encoder: BackwardDifferenceEncoder - Scaler: Normalizer - Shape: (130560, 40)\n",
      "35 -> Encoder: BackwardDifferenceEncoder - Scaler: PowerTransformer - Shape: (130560, 40)\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar las combinaciones de Encoder y Scaler\n",
    "encoder_methods = ['LabelEncoder', 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', 'BinaryEncoder', 'BackwardDifferenceEncoder']\n",
    "scaler_methods = ['StandardScaler', 'MinMaxScaler', 'MaxAbsScaler', 'RobustScaler', 'Normalizer', 'PowerTransformer']\n",
    "\n",
    "# 5. Instanciar CategoricalEncoders\n",
    "categorical = CategoricalEncoders(dataset=df_categorical)\n",
    "binary_columns, categorical_columns = categorical.get_binary_categorical_columns()\n",
    "\n",
    "# 6. Instanciar NumericalScalers\n",
    "numerical = NumericalScalers(dataset=df_numeric)\n",
    "\n",
    "# 7. Generar todas las combinaciones de Encoder y Scaler\n",
    "def get_list_data_processed(encoder_methods, scaler_methods):\n",
    "    combinations = list(itertools.product(encoder_methods, scaler_methods))\n",
    "    \n",
    "    i=0\n",
    "    list_data_processed = []\n",
    "    for encoder_method, scaler_method in combinations:\n",
    "        data_encoded = categorical.provider(binary_columns, categorical_columns, method=encoder_method)\n",
    "        data_scaled = numerical.provider(method=scaler_method)\n",
    "\n",
    "        processed_data = pd.concat([data_encoded, data_scaled], axis=1)\n",
    "        list_data_processed.append((f'{encoder_method} - {scaler_method}', processed_data))\n",
    "        print(f'{str(i).zfill(2)} -> Encoder: {encoder_method} - Scaler: {scaler_method} - Shape: {processed_data.shape}')\n",
    "        i+=1\n",
    "    \n",
    "    return list_data_processed\n",
    "\n",
    "list_data_processed = get_list_data_processed(encoder_methods, scaler_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split test y train de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Split Encoded Scaler Method: LabelEncoder - StandardScaler\n",
      "01 -> Split Encoded Scaler Method: LabelEncoder - MinMaxScaler\n",
      "02 -> Split Encoded Scaler Method: LabelEncoder - MaxAbsScaler\n",
      "03 -> Split Encoded Scaler Method: LabelEncoder - RobustScaler\n",
      "04 -> Split Encoded Scaler Method: LabelEncoder - Normalizer\n",
      "05 -> Split Encoded Scaler Method: LabelEncoder - PowerTransformer\n",
      "06 -> Split Encoded Scaler Method: OneHotEncoder - StandardScaler\n",
      "07 -> Split Encoded Scaler Method: OneHotEncoder - MinMaxScaler\n",
      "08 -> Split Encoded Scaler Method: OneHotEncoder - MaxAbsScaler\n",
      "09 -> Split Encoded Scaler Method: OneHotEncoder - RobustScaler\n",
      "10 -> Split Encoded Scaler Method: OneHotEncoder - Normalizer\n",
      "11 -> Split Encoded Scaler Method: OneHotEncoder - PowerTransformer\n",
      "12 -> Split Encoded Scaler Method: OrdinalEncoder - StandardScaler\n",
      "13 -> Split Encoded Scaler Method: OrdinalEncoder - MinMaxScaler\n",
      "14 -> Split Encoded Scaler Method: OrdinalEncoder - MaxAbsScaler\n",
      "15 -> Split Encoded Scaler Method: OrdinalEncoder - RobustScaler\n",
      "16 -> Split Encoded Scaler Method: OrdinalEncoder - Normalizer\n",
      "17 -> Split Encoded Scaler Method: OrdinalEncoder - PowerTransformer\n",
      "18 -> Split Encoded Scaler Method: FrequencyEncoder - StandardScaler\n",
      "19 -> Split Encoded Scaler Method: FrequencyEncoder - MinMaxScaler\n",
      "20 -> Split Encoded Scaler Method: FrequencyEncoder - MaxAbsScaler\n",
      "21 -> Split Encoded Scaler Method: FrequencyEncoder - RobustScaler\n",
      "22 -> Split Encoded Scaler Method: FrequencyEncoder - Normalizer\n",
      "23 -> Split Encoded Scaler Method: FrequencyEncoder - PowerTransformer\n",
      "24 -> Split Encoded Scaler Method: BinaryEncoder - StandardScaler\n",
      "25 -> Split Encoded Scaler Method: BinaryEncoder - MinMaxScaler\n",
      "26 -> Split Encoded Scaler Method: BinaryEncoder - MaxAbsScaler\n",
      "27 -> Split Encoded Scaler Method: BinaryEncoder - RobustScaler\n",
      "28 -> Split Encoded Scaler Method: BinaryEncoder - Normalizer\n",
      "29 -> Split Encoded Scaler Method: BinaryEncoder - PowerTransformer\n",
      "30 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - StandardScaler\n",
      "31 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MinMaxScaler\n",
      "32 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MaxAbsScaler\n",
      "33 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - RobustScaler\n",
      "34 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - Normalizer\n",
      "35 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - PowerTransformer\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar los conjuntos de entrenamiento y prueba\n",
    "def get_list_split_data(list_data_processed):\n",
    "    i=0\n",
    "    list_split_data = []\n",
    "    for encoded_scaler, data_processed in list_data_processed:\n",
    "        X = data_processed\n",
    "        y = df_target\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        list_split_data.append((encoded_scaler, X_train, X_test, y_train, y_test))\n",
    "        print(f'{str(i).zfill(2)} -> Split Encoded Scaler Method: {encoded_scaler}')\n",
    "        i+=1\n",
    "\n",
    "    return list_split_data\n",
    "\n",
    "list_split_data = get_list_split_data(list_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenar con la lista de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with logistic_regression: 0.807\n",
      "AUC on testing data with logistic_regression: 0.807\n",
      "Training time for logistic_regression: 0.248 seconds\n",
      "\n",
      "01 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.691\n",
      "Training time for decision_tree: 0.386 seconds\n",
      "\n",
      "02 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.366 seconds\n",
      "\n",
      "03 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with gradient_boosting: 0.844\n",
      "AUC on testing data with gradient_boosting: 0.842\n",
      "Training time for gradient_boosting: 6.215 seconds\n",
      "\n",
      "04 -> Model: knn\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with knn: 0.874\n",
      "AUC on testing data with knn: 0.791\n",
      "Training time for knn: 0.035 seconds\n",
      "\n",
      "05 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with naive_bayes: 0.788\n",
      "AUC on testing data with naive_bayes: 0.788\n",
      "Training time for naive_bayes: 0.011 seconds\n",
      "\n",
      "06 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with mlp: 0.843\n",
      "AUC on testing data with mlp: 0.836\n",
      "Training time for mlp: 24.968 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1061\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "07 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.850 seconds\n",
      "\n",
      "08 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with catboost: 0.873\n",
      "AUC on testing data with catboost: 0.852\n",
      "Training time for catboost: 6.292 seconds\n",
      "\n",
      "09 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.849\n",
      "Training time for xgboost: 0.184 seconds\n",
      "\n",
      "10 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.806\n",
      "AUC on testing data with logistic_regression: 0.806\n",
      "Training time for logistic_regression: 0.493 seconds\n",
      "\n",
      "11 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.684\n",
      "Training time for decision_tree: 0.377 seconds\n",
      "\n",
      "12 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.836\n",
      "Training time for random_forest: 6.590 seconds\n",
      "\n",
      "13 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with gradient_boosting: 0.841\n",
      "AUC on testing data with gradient_boosting: 0.840\n",
      "Training time for gradient_boosting: 6.415 seconds\n",
      "\n",
      "14 -> Model: knn\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with knn: 0.875\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.035 seconds\n",
      "\n",
      "15 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with naive_bayes: 0.788\n",
      "AUC on testing data with naive_bayes: 0.788\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n",
      "16 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with mlp: 0.840\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 27.459 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1075\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "17 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.849\n",
      "Training time for lgbm: 0.908 seconds\n",
      "\n",
      "18 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.851\n",
      "Training time for catboost: 6.215 seconds\n",
      "\n",
      "19 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.849\n",
      "Training time for xgboost: 0.182 seconds\n",
      "\n",
      "20 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.806\n",
      "AUC on testing data with logistic_regression: 0.806\n",
      "Training time for logistic_regression: 0.464 seconds\n",
      "\n",
      "21 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.684\n",
      "Training time for decision_tree: 0.382 seconds\n",
      "\n",
      "22 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.836\n",
      "Training time for random_forest: 6.552 seconds\n",
      "\n",
      "23 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with gradient_boosting: 0.841\n",
      "AUC on testing data with gradient_boosting: 0.840\n",
      "Training time for gradient_boosting: 6.414 seconds\n",
      "\n",
      "24 -> Model: knn\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with knn: 0.874\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.036 seconds\n",
      "\n",
      "25 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with naive_bayes: 0.788\n",
      "AUC on testing data with naive_bayes: 0.788\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n",
      "26 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with mlp: 0.837\n",
      "AUC on testing data with mlp: 0.836\n",
      "Training time for mlp: 790.091 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1078\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "27 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.849\n",
      "Training time for lgbm: 0.874 seconds\n",
      "\n",
      "28 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with catboost: 0.871\n",
      "AUC on testing data with catboost: 0.850\n",
      "Training time for catboost: 6.173 seconds\n",
      "\n",
      "29 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.847\n",
      "Training time for xgboost: 0.175 seconds\n",
      "\n",
      "30 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with logistic_regression: 0.807\n",
      "AUC on testing data with logistic_regression: 0.807\n",
      "Training time for logistic_regression: 0.211 seconds\n",
      "\n",
      "31 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.692\n",
      "Training time for decision_tree: 0.365 seconds\n",
      "\n",
      "32 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.839\n",
      "Training time for random_forest: 6.388 seconds\n",
      "\n",
      "33 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with gradient_boosting: 0.844\n",
      "AUC on testing data with gradient_boosting: 0.843\n",
      "Training time for gradient_boosting: 6.210 seconds\n",
      "\n",
      "34 -> Model: knn\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with knn: 0.875\n",
      "AUC on testing data with knn: 0.791\n",
      "Training time for knn: 0.034 seconds\n",
      "\n",
      "35 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with naive_bayes: 0.788\n",
      "AUC on testing data with naive_bayes: 0.788\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n",
      "36 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with mlp: 0.843\n",
      "AUC on testing data with mlp: 0.837\n",
      "Training time for mlp: 26.098 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1089\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "37 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.849\n",
      "Training time for lgbm: 0.922 seconds\n",
      "\n",
      "38 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with catboost: 0.873\n",
      "AUC on testing data with catboost: 0.852\n",
      "Training time for catboost: 6.181 seconds\n",
      "\n",
      "39 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.849\n",
      "Training time for xgboost: 0.160 seconds\n",
      "\n",
      "40 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with logistic_regression: 0.750\n",
      "AUC on testing data with logistic_regression: 0.751\n",
      "Training time for logistic_regression: 0.481 seconds\n",
      "\n",
      "41 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.687\n",
      "Training time for decision_tree: 0.868 seconds\n",
      "\n",
      "42 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with random_forest: 0.974\n",
      "AUC on testing data with random_forest: 0.828\n",
      "Training time for random_forest: 13.157 seconds\n",
      "\n",
      "43 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with gradient_boosting: 0.826\n",
      "AUC on testing data with gradient_boosting: 0.822\n",
      "Training time for gradient_boosting: 15.951 seconds\n",
      "\n",
      "44 -> Model: knn\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with knn: 0.862\n",
      "AUC on testing data with knn: 0.769\n",
      "Training time for knn: 0.034 seconds\n",
      "\n",
      "45 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with naive_bayes: 0.730\n",
      "AUC on testing data with naive_bayes: 0.733\n",
      "Training time for naive_bayes: 0.011 seconds\n",
      "\n",
      "46 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with mlp: 0.820\n",
      "AUC on testing data with mlp: 0.815\n",
      "Training time for mlp: 682.227 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1822\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "47 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with lgbm: 0.846\n",
      "AUC on testing data with lgbm: 0.835\n",
      "Training time for lgbm: 0.938 seconds\n",
      "\n",
      "48 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with catboost: 0.865\n",
      "AUC on testing data with catboost: 0.837\n",
      "Training time for catboost: 6.394 seconds\n",
      "\n",
      "49 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with xgboost: 0.871\n",
      "AUC on testing data with xgboost: 0.835\n",
      "Training time for xgboost: 0.230 seconds\n",
      "\n",
      "50 -> Model: logistic_regression\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.815\n",
      "AUC on testing data with logistic_regression: 0.814\n",
      "Training time for logistic_regression: 0.236 seconds\n",
      "\n",
      "51 -> Model: decision_tree\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.688\n",
      "Training time for decision_tree: 0.381 seconds\n",
      "\n",
      "52 -> Model: random_forest\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.837\n",
      "Training time for random_forest: 6.491 seconds\n",
      "\n",
      "53 -> Model: gradient_boosting\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with gradient_boosting: 0.842\n",
      "AUC on testing data with gradient_boosting: 0.841\n",
      "Training time for gradient_boosting: 6.261 seconds\n",
      "\n",
      "54 -> Model: knn\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with knn: 0.877\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.037 seconds\n",
      "\n",
      "55 -> Model: naive_bayes\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with naive_bayes: 0.796\n",
      "AUC on testing data with naive_bayes: 0.797\n",
      "Training time for naive_bayes: 0.015 seconds\n",
      "\n",
      "56 -> Model: mlp\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with mlp: 0.842\n",
      "AUC on testing data with mlp: 0.837\n",
      "Training time for mlp: 15.667 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1061\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "57 -> Model: lgbm\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.887 seconds\n",
      "\n",
      "58 -> Model: catboost\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.852\n",
      "Training time for catboost: 5.987 seconds\n",
      "\n",
      "59 -> Model: xgboost\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.848\n",
      "Training time for xgboost: 0.157 seconds\n",
      "\n",
      "60 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with logistic_regression: 0.827\n",
      "AUC on testing data with logistic_regression: 0.829\n",
      "Training time for logistic_regression: 0.368 seconds\n",
      "\n",
      "61 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.691\n",
      "Training time for decision_tree: 0.460 seconds\n",
      "\n",
      "62 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.255 seconds\n",
      "\n",
      "63 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with gradient_boosting: 0.845\n",
      "AUC on testing data with gradient_boosting: 0.844\n",
      "Training time for gradient_boosting: 7.669 seconds\n",
      "\n",
      "64 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with knn: 0.877\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.013 seconds\n",
      "\n",
      "65 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with naive_bayes: 0.770\n",
      "AUC on testing data with naive_bayes: 0.773\n",
      "Training time for naive_bayes: 0.028 seconds\n",
      "\n",
      "66 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with mlp: 0.855\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 40.933 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1084\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "67 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.909 seconds\n",
      "\n",
      "68 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with catboost: 0.874\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.006 seconds\n",
      "\n",
      "69 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.227 seconds\n",
      "\n",
      "70 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.827\n",
      "AUC on testing data with logistic_regression: 0.829\n",
      "Training time for logistic_regression: 0.639 seconds\n",
      "\n",
      "71 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.691\n",
      "Training time for decision_tree: 0.485 seconds\n",
      "\n",
      "72 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.837\n",
      "Training time for random_forest: 6.324 seconds\n",
      "\n",
      "73 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with gradient_boosting: 0.842\n",
      "AUC on testing data with gradient_boosting: 0.841\n",
      "Training time for gradient_boosting: 7.871 seconds\n",
      "\n",
      "74 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with knn: 0.874\n",
      "AUC on testing data with knn: 0.792\n",
      "Training time for knn: 0.013 seconds\n",
      "\n",
      "75 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with naive_bayes: 0.770\n",
      "AUC on testing data with naive_bayes: 0.773\n",
      "Training time for naive_bayes: 0.026 seconds\n",
      "\n",
      "76 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with mlp: 0.852\n",
      "AUC on testing data with mlp: 0.839\n",
      "Training time for mlp: 71.830 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "77 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.935 seconds\n",
      "\n",
      "78 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with catboost: 0.873\n",
      "AUC on testing data with catboost: 0.852\n",
      "Training time for catboost: 6.371 seconds\n",
      "\n",
      "79 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with xgboost: 0.879\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.271 seconds\n",
      "\n",
      "80 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.827\n",
      "AUC on testing data with logistic_regression: 0.829\n",
      "Training time for logistic_regression: 0.718 seconds\n",
      "\n",
      "81 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.691\n",
      "Training time for decision_tree: 0.499 seconds\n",
      "\n",
      "82 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.837\n",
      "Training time for random_forest: 7.021 seconds\n",
      "\n",
      "83 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with gradient_boosting: 0.842\n",
      "AUC on testing data with gradient_boosting: 0.841\n",
      "Training time for gradient_boosting: 8.193 seconds\n",
      "\n",
      "84 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with knn: 0.874\n",
      "AUC on testing data with knn: 0.792\n",
      "Training time for knn: 0.013 seconds\n",
      "\n",
      "85 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with naive_bayes: 0.770\n",
      "AUC on testing data with naive_bayes: 0.773\n",
      "Training time for naive_bayes: 0.027 seconds\n",
      "\n",
      "86 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with mlp: 0.851\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 60.283 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1101\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "87 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.932 seconds\n",
      "\n",
      "88 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.851\n",
      "Training time for catboost: 6.256 seconds\n",
      "\n",
      "89 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.848\n",
      "Training time for xgboost: 0.292 seconds\n",
      "\n",
      "90 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with logistic_regression: 0.827\n",
      "AUC on testing data with logistic_regression: 0.829\n",
      "Training time for logistic_regression: 0.660 seconds\n",
      "\n",
      "91 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.692\n",
      "Training time for decision_tree: 0.514 seconds\n",
      "\n",
      "92 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.839\n",
      "Training time for random_forest: 6.717 seconds\n",
      "\n",
      "93 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with gradient_boosting: 0.845\n",
      "AUC on testing data with gradient_boosting: 0.844\n",
      "Training time for gradient_boosting: 8.144 seconds\n",
      "\n",
      "94 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with knn: 0.876\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.013 seconds\n",
      "\n",
      "95 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with naive_bayes: 0.770\n",
      "AUC on testing data with naive_bayes: 0.773\n",
      "Training time for naive_bayes: 0.028 seconds\n",
      "\n",
      "96 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with mlp: 0.856\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 60.855 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "97 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.954 seconds\n",
      "\n",
      "98 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with catboost: 0.874\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.246 seconds\n",
      "\n",
      "99 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.260 seconds\n",
      "\n",
      "100 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with logistic_regression: 0.777\n",
      "AUC on testing data with logistic_regression: 0.780\n",
      "Training time for logistic_regression: 0.454 seconds\n",
      "\n",
      "101 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.684\n",
      "Training time for decision_tree: 1.040 seconds\n",
      "\n",
      "102 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with random_forest: 0.974\n",
      "AUC on testing data with random_forest: 0.831\n",
      "Training time for random_forest: 11.690 seconds\n",
      "\n",
      "103 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with gradient_boosting: 0.827\n",
      "AUC on testing data with gradient_boosting: 0.823\n",
      "Training time for gradient_boosting: 18.130 seconds\n",
      "\n",
      "104 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with knn: 0.862\n",
      "AUC on testing data with knn: 0.768\n",
      "Training time for knn: 0.014 seconds\n",
      "\n",
      "105 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with naive_bayes: 0.739\n",
      "AUC on testing data with naive_bayes: 0.743\n",
      "Training time for naive_bayes: 0.029 seconds\n",
      "\n",
      "106 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with mlp: 0.836\n",
      "AUC on testing data with mlp: 0.822\n",
      "Training time for mlp: 53.902 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1845\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "107 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with lgbm: 0.846\n",
      "AUC on testing data with lgbm: 0.835\n",
      "Training time for lgbm: 0.887 seconds\n",
      "\n",
      "108 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with catboost: 0.866\n",
      "AUC on testing data with catboost: 0.838\n",
      "Training time for catboost: 6.397 seconds\n",
      "\n",
      "109 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with xgboost: 0.870\n",
      "AUC on testing data with xgboost: 0.835\n",
      "Training time for xgboost: 0.320 seconds\n",
      "\n",
      "110 -> Model: logistic_regression\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.831\n",
      "AUC on testing data with logistic_regression: 0.832\n",
      "Training time for logistic_regression: 0.358 seconds\n",
      "\n",
      "111 -> Model: decision_tree\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.693\n",
      "Training time for decision_tree: 0.531 seconds\n",
      "\n",
      "112 -> Model: random_forest\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.716 seconds\n",
      "\n",
      "113 -> Model: gradient_boosting\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with gradient_boosting: 0.843\n",
      "AUC on testing data with gradient_boosting: 0.843\n",
      "Training time for gradient_boosting: 8.263 seconds\n",
      "\n",
      "114 -> Model: knn\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with knn: 0.878\n",
      "AUC on testing data with knn: 0.794\n",
      "Training time for knn: 0.012 seconds\n",
      "\n",
      "115 -> Model: naive_bayes\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with naive_bayes: 0.770\n",
      "AUC on testing data with naive_bayes: 0.773\n",
      "Training time for naive_bayes: 0.028 seconds\n",
      "\n",
      "116 -> Model: mlp\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with mlp: 0.856\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 64.772 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1084\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "117 -> Model: lgbm\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.897 seconds\n",
      "\n",
      "118 -> Model: catboost\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with catboost: 0.874\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.271 seconds\n",
      "\n",
      "119 -> Model: xgboost\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with xgboost: 0.878\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.236 seconds\n",
      "\n",
      "120 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with logistic_regression: 0.818\n",
      "AUC on testing data with logistic_regression: 0.819\n",
      "Training time for logistic_regression: 0.185 seconds\n",
      "\n",
      "121 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.693\n",
      "Training time for decision_tree: 0.358 seconds\n",
      "\n",
      "122 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.427 seconds\n",
      "\n",
      "123 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with gradient_boosting: 0.843\n",
      "AUC on testing data with gradient_boosting: 0.843\n",
      "Training time for gradient_boosting: 6.371 seconds\n",
      "\n",
      "124 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with knn: 0.878\n",
      "AUC on testing data with knn: 0.791\n",
      "Training time for knn: 0.035 seconds\n",
      "\n",
      "125 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with naive_bayes: 0.798\n",
      "AUC on testing data with naive_bayes: 0.799\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n",
      "126 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with mlp: 0.842\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 19.545 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1067\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "127 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.862 seconds\n",
      "\n",
      "128 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.408 seconds\n",
      "\n",
      "129 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.195 seconds\n",
      "\n",
      "130 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.818\n",
      "AUC on testing data with logistic_regression: 0.819\n",
      "Training time for logistic_regression: 0.470 seconds\n",
      "\n",
      "131 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.687\n",
      "Training time for decision_tree: 0.371 seconds\n",
      "\n",
      "132 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.836\n",
      "Training time for random_forest: 6.602 seconds\n",
      "\n",
      "133 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with gradient_boosting: 0.841\n",
      "AUC on testing data with gradient_boosting: 0.840\n",
      "Training time for gradient_boosting: 6.614 seconds\n",
      "\n",
      "134 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with knn: 0.876\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.038 seconds\n",
      "\n",
      "135 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with naive_bayes: 0.798\n",
      "AUC on testing data with naive_bayes: 0.799\n",
      "Training time for naive_bayes: 0.013 seconds\n",
      "\n",
      "136 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with mlp: 0.838\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 15.519 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "137 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.884 seconds\n",
      "\n",
      "138 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with catboost: 0.871\n",
      "AUC on testing data with catboost: 0.851\n",
      "Training time for catboost: 6.198 seconds\n",
      "\n",
      "139 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.167 seconds\n",
      "\n",
      "140 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.819\n",
      "AUC on testing data with logistic_regression: 0.819\n",
      "Training time for logistic_regression: 0.411 seconds\n",
      "\n",
      "141 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.687\n",
      "Training time for decision_tree: 0.367 seconds\n",
      "\n",
      "142 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.836\n",
      "Training time for random_forest: 6.477 seconds\n",
      "\n",
      "143 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with gradient_boosting: 0.841\n",
      "AUC on testing data with gradient_boosting: 0.840\n",
      "Training time for gradient_boosting: 6.467 seconds\n",
      "\n",
      "144 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with knn: 0.876\n",
      "AUC on testing data with knn: 0.793\n",
      "Training time for knn: 0.034 seconds\n",
      "\n",
      "145 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with naive_bayes: 0.798\n",
      "AUC on testing data with naive_bayes: 0.799\n",
      "Training time for naive_bayes: 0.015 seconds\n",
      "\n",
      "146 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with mlp: 0.838\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 15.681 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1084\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "147 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.897 seconds\n",
      "\n",
      "148 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with catboost: 0.871\n",
      "AUC on testing data with catboost: 0.850\n",
      "Training time for catboost: 6.079 seconds\n",
      "\n",
      "149 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with xgboost: 0.875\n",
      "AUC on testing data with xgboost: 0.848\n",
      "Training time for xgboost: 0.156 seconds\n",
      "\n",
      "150 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with logistic_regression: 0.818\n",
      "AUC on testing data with logistic_regression: 0.819\n",
      "Training time for logistic_regression: 0.214 seconds\n",
      "\n",
      "151 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.690\n",
      "Training time for decision_tree: 0.353 seconds\n",
      "\n",
      "152 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.523 seconds\n",
      "\n",
      "153 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with gradient_boosting: 0.844\n",
      "AUC on testing data with gradient_boosting: 0.844\n",
      "Training time for gradient_boosting: 6.479 seconds\n",
      "\n",
      "154 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with knn: 0.875\n",
      "AUC on testing data with knn: 0.790\n",
      "Training time for knn: 0.035 seconds\n",
      "\n",
      "155 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with naive_bayes: 0.798\n",
      "AUC on testing data with naive_bayes: 0.799\n",
      "Training time for naive_bayes: 0.014 seconds\n",
      "\n",
      "156 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with mlp: 0.842\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 24.575 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "157 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.867 seconds\n",
      "\n",
      "158 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.316 seconds\n",
      "\n",
      "159 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.849\n",
      "Training time for xgboost: 0.230 seconds\n",
      "\n",
      "160 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with logistic_regression: 0.769\n",
      "AUC on testing data with logistic_regression: 0.771\n",
      "Training time for logistic_regression: 0.503 seconds\n",
      "\n",
      "161 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.682\n",
      "Training time for decision_tree: 0.911 seconds\n",
      "\n",
      "162 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with random_forest: 0.974\n",
      "AUC on testing data with random_forest: 0.827\n",
      "Training time for random_forest: 13.668 seconds\n",
      "\n",
      "163 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with gradient_boosting: 0.826\n",
      "AUC on testing data with gradient_boosting: 0.823\n",
      "Training time for gradient_boosting: 16.206 seconds\n",
      "\n",
      "164 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with knn: 0.863\n",
      "AUC on testing data with knn: 0.769\n",
      "Training time for knn: 0.034 seconds\n",
      "\n",
      "165 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with naive_bayes: 0.743\n",
      "AUC on testing data with naive_bayes: 0.746\n",
      "Training time for naive_bayes: 0.014 seconds\n",
      "\n",
      "166 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with mlp: 0.823\n",
      "AUC on testing data with mlp: 0.820\n",
      "Training time for mlp: 41.263 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1828\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "167 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with lgbm: 0.845\n",
      "AUC on testing data with lgbm: 0.835\n",
      "Training time for lgbm: 0.879 seconds\n",
      "\n",
      "168 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with catboost: 0.864\n",
      "AUC on testing data with catboost: 0.838\n",
      "Training time for catboost: 6.390 seconds\n",
      "\n",
      "169 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with xgboost: 0.871\n",
      "AUC on testing data with xgboost: 0.835\n",
      "Training time for xgboost: 0.227 seconds\n",
      "\n",
      "170 -> Model: logistic_regression\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.822\n",
      "AUC on testing data with logistic_regression: 0.822\n",
      "Training time for logistic_regression: 0.196 seconds\n",
      "\n",
      "171 -> Model: decision_tree\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.686\n",
      "Training time for decision_tree: 0.361 seconds\n",
      "\n",
      "172 -> Model: random_forest\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.837\n",
      "Training time for random_forest: 6.766 seconds\n",
      "\n",
      "173 -> Model: gradient_boosting\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with gradient_boosting: 0.842\n",
      "AUC on testing data with gradient_boosting: 0.842\n",
      "Training time for gradient_boosting: 6.691 seconds\n",
      "\n",
      "174 -> Model: knn\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with knn: 0.878\n",
      "AUC on testing data with knn: 0.795\n",
      "Training time for knn: 0.043 seconds\n",
      "\n",
      "175 -> Model: naive_bayes\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with naive_bayes: 0.797\n",
      "AUC on testing data with naive_bayes: 0.799\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n",
      "176 -> Model: mlp\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with mlp: 0.842\n",
      "AUC on testing data with mlp: 0.838\n",
      "Training time for mlp: 23.218 seconds\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 48494, number of negative: 55954\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1067\n",
      "[LightGBM] [Info] Number of data points in the train set: 104448, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.464288 -> initscore=-0.143090\n",
      "[LightGBM] [Info] Start training from score -0.143090\n",
      "177 -> Model: lgbm\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with lgbm: 0.858\n",
      "AUC on testing data with lgbm: 0.850\n",
      "Training time for lgbm: 0.923 seconds\n",
      "\n",
      "178 -> Model: catboost\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with catboost: 0.872\n",
      "AUC on testing data with catboost: 0.853\n",
      "Training time for catboost: 6.088 seconds\n",
      "\n",
      "179 -> Model: xgboost\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with xgboost: 0.877\n",
      "AUC on testing data with xgboost: 0.850\n",
      "Training time for xgboost: 0.141 seconds\n",
      "\n",
      "180 -> Model: logistic_regression\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with logistic_regression: 0.811\n",
      "AUC on testing data with logistic_regression: 0.811\n",
      "Training time for logistic_regression: 0.144 seconds\n",
      "\n",
      "181 -> Model: decision_tree\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with decision_tree: 0.988\n",
      "AUC on testing data with decision_tree: 0.691\n",
      "Training time for decision_tree: 0.329 seconds\n",
      "\n",
      "182 -> Model: random_forest\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with random_forest: 0.975\n",
      "AUC on testing data with random_forest: 0.838\n",
      "Training time for random_forest: 6.262 seconds\n",
      "\n",
      "183 -> Model: gradient_boosting\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with gradient_boosting: 0.844\n",
      "AUC on testing data with gradient_boosting: 0.843\n",
      "Training time for gradient_boosting: 6.287 seconds\n",
      "\n",
      "184 -> Model: knn\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with knn: 0.874\n",
      "AUC on testing data with knn: 0.790\n",
      "Training time for knn: 0.039 seconds\n",
      "\n",
      "185 -> Model: naive_bayes\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with naive_bayes: 0.786\n",
      "AUC on testing data with naive_bayes: 0.786\n",
      "Training time for naive_bayes: 0.012 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicializar BaseModels y definir los nombres de los modelos\n",
    "base_models = BaseModels()\n",
    "name_models = ['logistic_regression', 'decision_tree', 'random_forest',\n",
    "               'gradient_boosting', 'knn', 'naive_bayes',\n",
    "               'mlp', 'lgbm', 'catboost', 'xgboost']\n",
    "\n",
    "# Iterar sobre los conjuntos de datos codificados\n",
    "i = 0\n",
    "all_results = []\n",
    "for encoded_scalar_method, X_train, X_test, y_train, y_test in list_split_data:\n",
    "    for name in name_models:\n",
    "        model = base_models.provider(name)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        predict_train = model.predict_proba(X_train)[:, 1]\n",
    "        predict_test = model.predict_proba(X_test)[:, 1]\n",
    "        predict_test_class = model.predict(X_test)\n",
    "\n",
    "        train_auc = roc_auc_score(y_train, predict_train)\n",
    "        test_auc = roc_auc_score(y_test, predict_test)\n",
    "\n",
    "        all_results.append((encoded_scalar_method, name, train_auc, test_auc, y_test, predict_test, predict_test_class))\n",
    "\n",
    "        print(f'{str(i).zfill(2)} -> Model: {name}')\n",
    "        print(f\"Encoded Method: {encoded_scalar_method} - AUC on training data with {name}: {train_auc:.3f}\")\n",
    "        print(f\"AUC on testing data with {name}: {test_auc:.3f}\")\n",
    "        print(f\"Training time for {name}: {elapsed_time:.3f} seconds\\n\")\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Graficar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot_results(all_results, index):\n",
    "    encoded_scalar_method, name, _, _, y_test, predict_test, predict_test_class = all_results[index]\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predict_test_class)\n",
    "    class_report = classification_report(y_test, predict_test_class)\n",
    "    auc_score = roc_auc_score(y_test, predict_test)\n",
    "    accuracy = accuracy_score(y_test, predict_test_class)\n",
    "    gini_score = 2 * auc_score - 1\n",
    "\n",
    "    # Print metrics and classification report\n",
    "    print(f\"Model: {name} - Encoded Scalar method {encoded_scalar_method}\")\n",
    "    print(f\"AUC Score:\\t{auc_score:.3f}\")\n",
    "    print(f\"Gini Score:\\t{gini_score:.3f}\")\n",
    "    print(f\"Accuracy:\\t{accuracy:.3f}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "    # Crear subplots 1x2\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "    # Plot confusion matrix with absolute values\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', cbar=False, ax=ax[0])\n",
    "    ax[0].set_xlabel('Predicted Label')\n",
    "    ax[0].set_ylabel('True Label')\n",
    "    ax[0].set_title('Absolute Values')\n",
    "\n",
    "    # Plot confusion matrix with percentages\n",
    "    conf_matrix_percent = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "    sns.heatmap(conf_matrix_percent, annot=True, fmt=\".2f\", cmap='Blues', cbar=False, ax=ax[1])\n",
    "    ax[1].set_xlabel('Predicted Label')\n",
    "    ax[1].set_ylabel('True Label')\n",
    "    ax[1].set_title('Percentages')\n",
    "\n",
    "    # Mostrar los gráficos\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot_results(all_results, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Consolidar todos los resultados en una lista\n",
    "consolidated_results = []\n",
    "for encoded_scalar_method, name, train_auc, test_auc, y_test, predict_test, predict_test_class in all_results:\n",
    "    full_model_name = f\"{encoded_scalar_method} - {name}\"\n",
    "    test_accuracy = accuracy_score(y_test, predict_test_class)\n",
    "    gini_score = 2 * test_auc - 1\n",
    "    consolidated_results.append((full_model_name, train_auc, test_auc, test_accuracy, gini_score))\n",
    "\n",
    "# Ordenar los resultados por test_auc de mayor a menor y seleccionar el top 10\n",
    "consolidated_results.sort(key=lambda x: x[2], reverse=False)\n",
    "top_10_results = consolidated_results[-10:]\n",
    "\n",
    "# Desempaquetar los resultados del top 10\n",
    "top_10_names, top_10_train_aucs, top_10_test_aucs, top_10_accuracies, top_10_ginis = zip(*top_10_results)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Gráfico de líneas para el top 10\n",
    "plt.plot(top_10_names, top_10_train_aucs, label='Training AUC', marker='o', color='skyblue')\n",
    "plt.plot(top_10_names, top_10_test_aucs, label='Testing AUC', marker='o', color='salmon')\n",
    "plt.plot(top_10_names, top_10_accuracies, label='Testing Accuracy', marker='o', color='green')\n",
    "plt.plot(top_10_names, top_10_ginis, label='Testing Gini', marker='o', color='purple')\n",
    "\n",
    "# Añadir los valores a los puntos\n",
    "for i, txt in enumerate(top_10_train_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_train_aucs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "for i, txt in enumerate(top_10_test_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_test_aucs[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='red')\n",
    "for i, txt in enumerate(top_10_accuracies):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_accuracies[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='green')\n",
    "for i, txt in enumerate(top_10_ginis):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_ginis[i]), textcoords=\"offset points\", xytext=(0,-25), ha='center', color='purple')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Métricas')\n",
    "plt.title('Top 10 Models by Testing AUC, Accuracy, and Gini')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

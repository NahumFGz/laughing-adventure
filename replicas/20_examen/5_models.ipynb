{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.base_models import BaseModels\n",
    "from utils.numerical_scalers import NumericalScalers\n",
    "from utils.categorical_encoders import CategoricalEncoders\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Leer datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FBK_ULT12</th>\n",
       "      <th>FBK_BEST12</th>\n",
       "      <th>COD_SALA</th>\n",
       "      <th>DEPARTAMENTO</th>\n",
       "      <th>SEGMENTO</th>\n",
       "      <th>RANGO_INGRESOS</th>\n",
       "      <th>NUMPRIORIZACION</th>\n",
       "      <th>NC_DISTR12</th>\n",
       "      <th>TOTGEST6</th>\n",
       "      <th>DIAS_ULT6</th>\n",
       "      <th>DIAS_BEST12</th>\n",
       "      <th>NC_CTD12</th>\n",
       "      <th>NC_DIAS6</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>EC</td>\n",
       "      <td>FUERA DE LIMA</td>\n",
       "      <td>2</td>\n",
       "      <td>Entre S/.4000-10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>PP</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>1BC</td>\n",
       "      <td>Entre S/.4000-10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>PA</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>2</td>\n",
       "      <td>Entre S/.1000-4000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>NC</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>2</td>\n",
       "      <td>Entre S/.1000-4000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587062</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>EC</td>\n",
       "      <td>FUERA DE LIMA</td>\n",
       "      <td>3</td>\n",
       "      <td>Entre S/.4000-10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305600</th>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>NC</td>\n",
       "      <td>FUERA DE LIMA</td>\n",
       "      <td>5</td>\n",
       "      <td>Entre S/.600-1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587062</td>\n",
       "      <td>7.463895</td>\n",
       "      <td>44.627989</td>\n",
       "      <td>80.041789</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305601</th>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>2DA</td>\n",
       "      <td>FUERA DE LIMA</td>\n",
       "      <td>1BC</td>\n",
       "      <td>Entre S/.4000-10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587062</td>\n",
       "      <td>7.463895</td>\n",
       "      <td>44.627989</td>\n",
       "      <td>80.041789</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305602</th>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>NC</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>3</td>\n",
       "      <td>Sin ingresos</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587062</td>\n",
       "      <td>7.463895</td>\n",
       "      <td>44.627989</td>\n",
       "      <td>80.041789</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305603</th>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>SIN DATOS</td>\n",
       "      <td>NC</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>3</td>\n",
       "      <td>Entre S/.1000-4000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.587062</td>\n",
       "      <td>7.463895</td>\n",
       "      <td>44.627989</td>\n",
       "      <td>80.041789</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305604</th>\n",
       "      <td>TLV</td>\n",
       "      <td>TLV</td>\n",
       "      <td>IL</td>\n",
       "      <td>LIMA</td>\n",
       "      <td>1BC</td>\n",
       "      <td>Entre S/.4000-10000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1305605 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FBK_ULT12 FBK_BEST12 COD_SALA   DEPARTAMENTO SEGMENTO  \\\n",
       "0              TLV        TLV       EC  FUERA DE LIMA        2   \n",
       "1              TLV        TLV       PP           LIMA      1BC   \n",
       "2              TLV        TLV       PA           LIMA        2   \n",
       "3              TLV        TLV       NC           LIMA        2   \n",
       "4              TLV        TLV       EC  FUERA DE LIMA        3   \n",
       "...            ...        ...      ...            ...      ...   \n",
       "1305600  SIN DATOS  SIN DATOS       NC  FUERA DE LIMA        5   \n",
       "1305601  SIN DATOS  SIN DATOS      2DA  FUERA DE LIMA      1BC   \n",
       "1305602  SIN DATOS  SIN DATOS       NC           LIMA        3   \n",
       "1305603  SIN DATOS  SIN DATOS       NC           LIMA        3   \n",
       "1305604        TLV        TLV       IL           LIMA      1BC   \n",
       "\n",
       "              RANGO_INGRESOS  NUMPRIORIZACION  NC_DISTR12   TOTGEST6  \\\n",
       "0        Entre S/.4000-10000                1    0.333333   6.000000   \n",
       "1        Entre S/.4000-10000                1    0.461538  13.000000   \n",
       "2         Entre S/.1000-4000                1    0.666667   2.000000   \n",
       "3         Entre S/.1000-4000                1    0.587062   4.000000   \n",
       "4        Entre S/.4000-10000                1    0.187500  10.000000   \n",
       "...                      ...              ...         ...        ...   \n",
       "1305600    Entre S/.600-1000                1    0.587062   7.463895   \n",
       "1305601  Entre S/.4000-10000                1    0.587062   7.463895   \n",
       "1305602         Sin ingresos                1    0.587062   7.463895   \n",
       "1305603   Entre S/.1000-4000                2    0.587062   7.463895   \n",
       "1305604  Entre S/.4000-10000                2    1.000000   1.000000   \n",
       "\n",
       "          DIAS_ULT6  DIAS_BEST12  NC_CTD12  NC_DIAS6  TARGET  \n",
       "0          8.000000     8.000000       2.0      58.0       1  \n",
       "1          9.000000    46.000000       6.0       9.0       1  \n",
       "2        127.000000   223.000000       4.0     127.0       1  \n",
       "3         49.000000    49.000000       4.0      38.0       1  \n",
       "4         27.000000    27.000000       3.0     140.0       1  \n",
       "...             ...          ...       ...       ...     ...  \n",
       "1305600   44.627989    80.041789       4.0      38.0       0  \n",
       "1305601   44.627989    80.041789       4.0      38.0       0  \n",
       "1305602   44.627989    80.041789       4.0      38.0       1  \n",
       "1305603   44.627989    80.041789       4.0      38.0       0  \n",
       "1305604  196.000000   196.000000       1.0     196.0       0  \n",
       "\n",
       "[1305605 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Leer el dataset\n",
    "dataset = pd.read_parquet('./data/2_data_preprocesada.parquet')\n",
    "dataset = dataset.drop(columns=['DF_TYPE'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generar combinaciones de categóricos y numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separar TARGET, df_categorical y df_numeric\n",
    "df_target = dataset['TARGET']\n",
    "df_categorical = dataset.select_dtypes(include=['object'])\n",
    "df_numeric = dataset.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# 3. Eliminar TARGET de df_numeric\n",
    "df_numeric = df_numeric.drop(columns=['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Encoder: LabelEncoder - Scaler: StandardScaler - Shape: (1305605, 13)\n",
      "01 -> Encoder: LabelEncoder - Scaler: MinMaxScaler - Shape: (1305605, 13)\n",
      "02 -> Encoder: LabelEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 13)\n",
      "03 -> Encoder: LabelEncoder - Scaler: RobustScaler - Shape: (1305605, 13)\n",
      "04 -> Encoder: LabelEncoder - Scaler: Normalizer - Shape: (1305605, 13)\n",
      "05 -> Encoder: LabelEncoder - Scaler: PowerTransformer - Shape: (1305605, 13)\n",
      "06 -> Encoder: OneHotEncoder - Scaler: StandardScaler - Shape: (1305605, 39)\n",
      "07 -> Encoder: OneHotEncoder - Scaler: MinMaxScaler - Shape: (1305605, 39)\n",
      "08 -> Encoder: OneHotEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 39)\n",
      "09 -> Encoder: OneHotEncoder - Scaler: RobustScaler - Shape: (1305605, 39)\n",
      "10 -> Encoder: OneHotEncoder - Scaler: Normalizer - Shape: (1305605, 39)\n",
      "11 -> Encoder: OneHotEncoder - Scaler: PowerTransformer - Shape: (1305605, 39)\n",
      "12 -> Encoder: OrdinalEncoder - Scaler: StandardScaler - Shape: (1305605, 13)\n",
      "13 -> Encoder: OrdinalEncoder - Scaler: MinMaxScaler - Shape: (1305605, 13)\n",
      "14 -> Encoder: OrdinalEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 13)\n",
      "15 -> Encoder: OrdinalEncoder - Scaler: RobustScaler - Shape: (1305605, 13)\n",
      "16 -> Encoder: OrdinalEncoder - Scaler: Normalizer - Shape: (1305605, 13)\n",
      "17 -> Encoder: OrdinalEncoder - Scaler: PowerTransformer - Shape: (1305605, 13)\n",
      "18 -> Encoder: FrequencyEncoder - Scaler: StandardScaler - Shape: (1305605, 13)\n",
      "19 -> Encoder: FrequencyEncoder - Scaler: MinMaxScaler - Shape: (1305605, 13)\n",
      "20 -> Encoder: FrequencyEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 13)\n",
      "21 -> Encoder: FrequencyEncoder - Scaler: RobustScaler - Shape: (1305605, 13)\n",
      "22 -> Encoder: FrequencyEncoder - Scaler: Normalizer - Shape: (1305605, 13)\n",
      "23 -> Encoder: FrequencyEncoder - Scaler: PowerTransformer - Shape: (1305605, 13)\n",
      "24 -> Encoder: BinaryEncoder - Scaler: StandardScaler - Shape: (1305605, 24)\n",
      "25 -> Encoder: BinaryEncoder - Scaler: MinMaxScaler - Shape: (1305605, 24)\n",
      "26 -> Encoder: BinaryEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 24)\n",
      "27 -> Encoder: BinaryEncoder - Scaler: RobustScaler - Shape: (1305605, 24)\n",
      "28 -> Encoder: BinaryEncoder - Scaler: Normalizer - Shape: (1305605, 24)\n",
      "29 -> Encoder: BinaryEncoder - Scaler: PowerTransformer - Shape: (1305605, 24)\n",
      "30 -> Encoder: BackwardDifferenceEncoder - Scaler: StandardScaler - Shape: (1305605, 40)\n",
      "31 -> Encoder: BackwardDifferenceEncoder - Scaler: MinMaxScaler - Shape: (1305605, 40)\n",
      "32 -> Encoder: BackwardDifferenceEncoder - Scaler: MaxAbsScaler - Shape: (1305605, 40)\n",
      "33 -> Encoder: BackwardDifferenceEncoder - Scaler: RobustScaler - Shape: (1305605, 40)\n",
      "34 -> Encoder: BackwardDifferenceEncoder - Scaler: Normalizer - Shape: (1305605, 40)\n",
      "35 -> Encoder: BackwardDifferenceEncoder - Scaler: PowerTransformer - Shape: (1305605, 40)\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar las combinaciones de Encoder y Scaler\n",
    "encoder_methods = ['LabelEncoder', 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', 'BinaryEncoder', 'BackwardDifferenceEncoder']\n",
    "scaler_methods = ['StandardScaler', 'MinMaxScaler', 'MaxAbsScaler', 'RobustScaler', 'Normalizer', 'PowerTransformer']\n",
    "\n",
    "# 5. Instanciar CategoricalEncoders\n",
    "categorical = CategoricalEncoders(dataset=df_categorical)\n",
    "binary_columns, categorical_columns = categorical.get_binary_categorical_columns()\n",
    "\n",
    "# 6. Instanciar NumericalScalers\n",
    "numerical = NumericalScalers(dataset=df_numeric)\n",
    "\n",
    "# 7. Generar todas las combinaciones de Encoder y Scaler\n",
    "def get_list_data_processed(encoder_methods, scaler_methods):\n",
    "    combinations = list(itertools.product(encoder_methods, scaler_methods))\n",
    "    \n",
    "    i=0\n",
    "    list_data_processed = []\n",
    "    for encoder_method, scaler_method in combinations:\n",
    "        data_encoded = categorical.provider(binary_columns, categorical_columns, method=encoder_method)\n",
    "        data_scaled = numerical.provider(method=scaler_method)\n",
    "\n",
    "        processed_data = pd.concat([data_encoded, data_scaled], axis=1)\n",
    "        list_data_processed.append((f'{encoder_method} - {scaler_method}', processed_data))\n",
    "        print(f'{str(i).zfill(2)} -> Encoder: {encoder_method} - Scaler: {scaler_method} - Shape: {processed_data.shape}')\n",
    "        i+=1\n",
    "    \n",
    "    return list_data_processed\n",
    "\n",
    "list_data_processed = get_list_data_processed(encoder_methods, scaler_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split test y train de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 -> Split Encoded Scaler Method: LabelEncoder - StandardScaler\n",
      "01 -> Split Encoded Scaler Method: LabelEncoder - MinMaxScaler\n",
      "02 -> Split Encoded Scaler Method: LabelEncoder - MaxAbsScaler\n",
      "03 -> Split Encoded Scaler Method: LabelEncoder - RobustScaler\n",
      "04 -> Split Encoded Scaler Method: LabelEncoder - Normalizer\n",
      "05 -> Split Encoded Scaler Method: LabelEncoder - PowerTransformer\n",
      "06 -> Split Encoded Scaler Method: OneHotEncoder - StandardScaler\n",
      "07 -> Split Encoded Scaler Method: OneHotEncoder - MinMaxScaler\n",
      "08 -> Split Encoded Scaler Method: OneHotEncoder - MaxAbsScaler\n",
      "09 -> Split Encoded Scaler Method: OneHotEncoder - RobustScaler\n",
      "10 -> Split Encoded Scaler Method: OneHotEncoder - Normalizer\n",
      "11 -> Split Encoded Scaler Method: OneHotEncoder - PowerTransformer\n",
      "12 -> Split Encoded Scaler Method: OrdinalEncoder - StandardScaler\n",
      "13 -> Split Encoded Scaler Method: OrdinalEncoder - MinMaxScaler\n",
      "14 -> Split Encoded Scaler Method: OrdinalEncoder - MaxAbsScaler\n",
      "15 -> Split Encoded Scaler Method: OrdinalEncoder - RobustScaler\n",
      "16 -> Split Encoded Scaler Method: OrdinalEncoder - Normalizer\n",
      "17 -> Split Encoded Scaler Method: OrdinalEncoder - PowerTransformer\n",
      "18 -> Split Encoded Scaler Method: FrequencyEncoder - StandardScaler\n",
      "19 -> Split Encoded Scaler Method: FrequencyEncoder - MinMaxScaler\n",
      "20 -> Split Encoded Scaler Method: FrequencyEncoder - MaxAbsScaler\n",
      "21 -> Split Encoded Scaler Method: FrequencyEncoder - RobustScaler\n",
      "22 -> Split Encoded Scaler Method: FrequencyEncoder - Normalizer\n",
      "23 -> Split Encoded Scaler Method: FrequencyEncoder - PowerTransformer\n",
      "24 -> Split Encoded Scaler Method: BinaryEncoder - StandardScaler\n",
      "25 -> Split Encoded Scaler Method: BinaryEncoder - MinMaxScaler\n",
      "26 -> Split Encoded Scaler Method: BinaryEncoder - MaxAbsScaler\n",
      "27 -> Split Encoded Scaler Method: BinaryEncoder - RobustScaler\n",
      "28 -> Split Encoded Scaler Method: BinaryEncoder - Normalizer\n",
      "29 -> Split Encoded Scaler Method: BinaryEncoder - PowerTransformer\n",
      "30 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - StandardScaler\n",
      "31 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MinMaxScaler\n",
      "32 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - MaxAbsScaler\n",
      "33 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - RobustScaler\n",
      "34 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - Normalizer\n",
      "35 -> Split Encoded Scaler Method: BackwardDifferenceEncoder - PowerTransformer\n"
     ]
    }
   ],
   "source": [
    "# 4. Generar los conjuntos de entrenamiento y prueba\n",
    "def get_list_split_data(list_data_processed):\n",
    "    i=0\n",
    "    list_split_data = []\n",
    "    for encoded_scaler, data_processed in list_data_processed:\n",
    "        X = data_processed\n",
    "        y = df_target\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        list_split_data.append((encoded_scaler, X_train, X_test, y_train, y_test))\n",
    "        print(f'{str(i).zfill(2)} -> Split Encoded Scaler Method: {encoded_scaler}')\n",
    "        i+=1\n",
    "\n",
    "    return list_split_data\n",
    "\n",
    "list_split_data = get_list_split_data(list_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenar con la lista de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 <->\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with logistic_regression: 0.805\n",
      "AUC on testing data with logistic_regression: 0.805\n",
      "\n",
      "01 <->\n",
      "Encoded Method: LabelEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.786\n",
      "\n",
      "02 <->\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.803\n",
      "AUC on testing data with logistic_regression: 0.803\n",
      "\n",
      "03 <->\n",
      "Encoded Method: LabelEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "04 <->\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.803\n",
      "AUC on testing data with logistic_regression: 0.803\n",
      "\n",
      "05 <->\n",
      "Encoded Method: LabelEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "06 <->\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with logistic_regression: 0.805\n",
      "AUC on testing data with logistic_regression: 0.805\n",
      "\n",
      "07 <->\n",
      "Encoded Method: LabelEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.788\n",
      "\n",
      "08 <->\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with logistic_regression: 0.751\n",
      "AUC on testing data with logistic_regression: 0.750\n",
      "\n",
      "09 <->\n",
      "Encoded Method: LabelEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.781\n",
      "\n",
      "10 <->\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.813\n",
      "AUC on testing data with logistic_regression: 0.813\n",
      "\n",
      "11 <->\n",
      "Encoded Method: LabelEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "12 <->\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "13 <->\n",
      "Encoded Method: OneHotEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "14 <->\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "15 <->\n",
      "Encoded Method: OneHotEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.786\n",
      "\n",
      "16 <->\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "17 <->\n",
      "Encoded Method: OneHotEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.786\n",
      "\n",
      "18 <->\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "19 <->\n",
      "Encoded Method: OneHotEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.788\n",
      "\n",
      "20 <->\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with logistic_regression: 0.779\n",
      "AUC on testing data with logistic_regression: 0.780\n",
      "\n",
      "21 <->\n",
      "Encoded Method: OneHotEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.782\n",
      "\n",
      "22 <->\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.830\n",
      "AUC on testing data with logistic_regression: 0.830\n",
      "\n",
      "23 <->\n",
      "Encoded Method: OneHotEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "24 <->\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with logistic_regression: 0.808\n",
      "AUC on testing data with logistic_regression: 0.808\n",
      "\n",
      "25 <->\n",
      "Encoded Method: OrdinalEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.786\n",
      "\n",
      "26 <->\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.807\n",
      "AUC on testing data with logistic_regression: 0.807\n",
      "\n",
      "27 <->\n",
      "Encoded Method: OrdinalEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "28 <->\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.807\n",
      "AUC on testing data with logistic_regression: 0.806\n",
      "\n",
      "29 <->\n",
      "Encoded Method: OrdinalEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "30 <->\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with logistic_regression: 0.808\n",
      "AUC on testing data with logistic_regression: 0.808\n",
      "\n",
      "31 <->\n",
      "Encoded Method: OrdinalEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.788\n",
      "\n",
      "32 <->\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with logistic_regression: 0.758\n",
      "AUC on testing data with logistic_regression: 0.758\n",
      "\n",
      "33 <->\n",
      "Encoded Method: OrdinalEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.781\n",
      "\n",
      "34 <->\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.816\n",
      "AUC on testing data with logistic_regression: 0.816\n",
      "\n",
      "35 <->\n",
      "Encoded Method: OrdinalEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "36 <->\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with logistic_regression: 0.809\n",
      "AUC on testing data with logistic_regression: 0.809\n",
      "\n",
      "37 <->\n",
      "Encoded Method: FrequencyEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "38 <->\n",
      "Encoded Method: FrequencyEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.809\n",
      "AUC on testing data with logistic_regression: 0.809\n",
      "\n",
      "39 <->\n",
      "Encoded Method: FrequencyEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.784\n",
      "\n",
      "40 <->\n",
      "Encoded Method: FrequencyEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.809\n",
      "AUC on testing data with logistic_regression: 0.809\n",
      "\n",
      "41 <->\n",
      "Encoded Method: FrequencyEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.784\n",
      "\n",
      "42 <->\n",
      "Encoded Method: FrequencyEncoder - RobustScaler - AUC on training data with logistic_regression: 0.809\n",
      "AUC on testing data with logistic_regression: 0.809\n",
      "\n",
      "43 <->\n",
      "Encoded Method: FrequencyEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "44 <->\n",
      "Encoded Method: FrequencyEncoder - Normalizer - AUC on training data with logistic_regression: 0.756\n",
      "AUC on testing data with logistic_regression: 0.755\n",
      "\n",
      "45 <->\n",
      "Encoded Method: FrequencyEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.781\n",
      "\n",
      "46 <->\n",
      "Encoded Method: FrequencyEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.816\n",
      "AUC on testing data with logistic_regression: 0.816\n",
      "\n",
      "47 <->\n",
      "Encoded Method: FrequencyEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "48 <->\n",
      "Encoded Method: BinaryEncoder - StandardScaler - AUC on training data with logistic_regression: 0.825\n",
      "AUC on testing data with logistic_regression: 0.825\n",
      "\n",
      "49 <->\n",
      "Encoded Method: BinaryEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "50 <->\n",
      "Encoded Method: BinaryEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.825\n",
      "AUC on testing data with logistic_regression: 0.825\n",
      "\n",
      "51 <->\n",
      "Encoded Method: BinaryEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "52 <->\n",
      "Encoded Method: BinaryEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.825\n",
      "AUC on testing data with logistic_regression: 0.825\n",
      "\n",
      "53 <->\n",
      "Encoded Method: BinaryEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "54 <->\n",
      "Encoded Method: BinaryEncoder - RobustScaler - AUC on training data with logistic_regression: 0.825\n",
      "AUC on testing data with logistic_regression: 0.825\n",
      "\n",
      "55 <->\n",
      "Encoded Method: BinaryEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.788\n",
      "\n",
      "56 <->\n",
      "Encoded Method: BinaryEncoder - Normalizer - AUC on training data with logistic_regression: 0.779\n",
      "AUC on testing data with logistic_regression: 0.779\n",
      "\n",
      "57 <->\n",
      "Encoded Method: BinaryEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.780\n",
      "\n",
      "58 <->\n",
      "Encoded Method: BinaryEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.828\n",
      "AUC on testing data with logistic_regression: 0.829\n",
      "\n",
      "59 <->\n",
      "Encoded Method: BinaryEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n",
      "60 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - StandardScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "61 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - StandardScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.786\n",
      "\n",
      "62 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - MinMaxScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "63 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - MinMaxScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "64 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - MaxAbsScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "65 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - MaxAbsScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.785\n",
      "\n",
      "66 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - RobustScaler - AUC on training data with logistic_regression: 0.826\n",
      "AUC on testing data with logistic_regression: 0.827\n",
      "\n",
      "67 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - RobustScaler - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.789\n",
      "\n",
      "68 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - Normalizer - AUC on training data with logistic_regression: 0.780\n",
      "AUC on testing data with logistic_regression: 0.780\n",
      "\n",
      "69 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - Normalizer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.781\n",
      "\n",
      "70 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - PowerTransformer - AUC on training data with logistic_regression: 0.830\n",
      "AUC on testing data with logistic_regression: 0.830\n",
      "\n",
      "71 <->\n",
      "Encoded Method: BackwardDifferenceEncoder - PowerTransformer - AUC on training data with decision_tree: 0.983\n",
      "AUC on testing data with decision_tree: 0.787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicializar BaseModels y definir los nombres de los modelos\n",
    "base_models = BaseModels()\n",
    "name_models = ['logistic_regression', 'decision_tree']\n",
    "\n",
    "# Variable para almacenar los resultados\n",
    "all_results = []\n",
    "\n",
    "# Iterar sobre los conjuntos de datos codificados\n",
    "i = 0\n",
    "for encoded_method, X_train, X_test, y_train, y_test in list_split_data:\n",
    "    results = []\n",
    "    for name in name_models:\n",
    "        model = base_models.provider(name)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predict_train = model.predict_proba(X_train)[:, 1]\n",
    "        predict_test = model.predict_proba(X_test)[:, 1]\n",
    "        predict_test_class = model.predict(X_test)\n",
    "\n",
    "        train_auc = roc_auc_score(y_train, predict_train)\n",
    "        test_auc = roc_auc_score(y_test, predict_test)\n",
    "\n",
    "        results.append((name, train_auc, test_auc, y_test, predict_test_class))\n",
    "\n",
    "        print(f'{str(i).zfill(2)} <->')\n",
    "        print(f\"Encoded Method: {encoded_method} - AUC on training data with {name}: {train_auc:.3f}\")\n",
    "        print(f\"AUC on testing data with {name}: {test_auc:.3f}\\n\")\n",
    "        i += 1\n",
    "\n",
    "    # Ordenar los resultados por test_auc de mayor a menor\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Desempaquetar los resultados ordenados y almacenar con el método de codificación\n",
    "    sorted_names, train_aucs, test_aucs, y_tests, predict_test_classes = zip(*results)\n",
    "    all_results.append((encoded_method, sorted_names, train_aucs, test_aucs, y_tests, predict_test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(all_results):\n",
    "    for encoded_method, sorted_names, train_aucs, test_aucs, _, _ in all_results:\n",
    "        # Ordenar los resultados por Testing AUC de menor a mayor\n",
    "        sorted_indices = np.argsort(test_aucs)\n",
    "        sorted_names = np.array(sorted_names)[sorted_indices]\n",
    "        train_aucs = np.array(train_aucs)[sorted_indices]\n",
    "        test_aucs = np.array(test_aucs)[sorted_indices]\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        # Gráfico de líneas\n",
    "        plt.plot(sorted_names, train_aucs, label='Training AUC', marker='o', color='skyblue')\n",
    "        plt.plot(sorted_names, test_aucs, label='Testing AUC', marker='o', color='salmon')\n",
    "\n",
    "        # Añadir los valores a los puntos\n",
    "        for i, txt in enumerate(train_aucs):\n",
    "            plt.annotate(f'{txt:.3f}', (sorted_names[i], train_aucs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "        for i, txt in enumerate(test_aucs):\n",
    "            plt.annotate(f'{txt:.3f}', (sorted_names[i], test_aucs[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='red')\n",
    "\n",
    "        plt.xlabel('Modelos')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title(f'AUC for Training and Testing - {encoded_method}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Llamar a la función para generar los gráficos\n",
    "plot_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidar todos los resultados en una lista\n",
    "consolidated_results = []\n",
    "for encoded_method, sorted_names, train_aucs, test_aucs, _, _ in all_results:\n",
    "    for name, train_auc, test_auc in zip(sorted_names, train_aucs, test_aucs):\n",
    "        full_model_name = f\"{encoded_method} - {name}\"\n",
    "        consolidated_results.append((full_model_name, train_auc, test_auc))\n",
    "\n",
    "# Ordenar los resultados por test_auc de mayor a menor y seleccionar el top 10\n",
    "consolidated_results.sort(key=lambda x: x[2], reverse=False)\n",
    "top_10_results = consolidated_results[-10:]\n",
    "\n",
    "# Desempaquetar los resultados del top 10\n",
    "top_10_names, top_10_train_aucs, top_10_test_aucs = zip(*top_10_results)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Gráfico de líneas para el top 10\n",
    "plt.plot(top_10_names, top_10_train_aucs, label='Training AUC', marker='o', color='skyblue')\n",
    "plt.plot(top_10_names, top_10_test_aucs, label='Testing AUC', marker='o', color='salmon')\n",
    "\n",
    "# Añadir los valores a los puntos\n",
    "for i, txt in enumerate(top_10_train_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_train_aucs[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='blue')\n",
    "for i, txt in enumerate(top_10_test_aucs):\n",
    "    plt.annotate(f'{txt:.3f}', (top_10_names[i], top_10_test_aucs[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='red')\n",
    "\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Top 10 Models by Testing AUC')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
